{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries and define custom functions for log returns, realized volatility, and RMSPE from tutorial notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train['target'] = np.sqrt(train['target'])\n",
    "book = pd.read_parquet('order_book_feature.parquet')\n",
    "trades = pd.read_parquet('trades.parquet')\n",
    "time_id_reference = pd.read_csv('time_id_reference.csv', parse_dates=['date'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering. Calculate 'wap', 'bid_ask_spread', and 'order_imbalance' in the order book. Calculate log returns for trades. \n",
    "Aggregate features for both book and trades data and merge with the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/81hq8_d56pjbxtvpn4bqw7hm0000gn/T/ipykernel_14742/1206922324.py:6: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  trades['price_log_return'] = trades.groupby(['time_id', 'stock_id'])['price'].apply(log_return)\n"
     ]
    }
   ],
   "source": [
    "book['wap'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) / (book['bid_size1'] + book['ask_size1'])\n",
    "book['bid_ask_spread'] = book['ask_price1'] - book['bid_price1']\n",
    "book['order_imbalance'] = (book['bid_size1'] - book['ask_size1']) / (book['bid_size1'] + book['ask_size1'])\n",
    "\n",
    "\n",
    "trades['price_log_return'] = trades.groupby(['time_id', 'stock_id'])['price'].apply(log_return)\n",
    "trades = trades.dropna()\n",
    "\n",
    "features = book.groupby(['time_id', 'stock_id']).agg({\n",
    "    'wap': 'mean',\n",
    "    'bid_ask_spread': np.var,\n",
    "    'order_imbalance': np.var,\n",
    "}).reset_index()\n",
    "\n",
    "trades_features = trades.groupby(['time_id', 'stock_id']).agg({\n",
    "    'price_log_return': [realized_volatility],\n",
    "    'size': 'sum',\n",
    "    'order_count': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "trades_features.columns = ['_'.join(col) if col[0] != 'time_id' and col[0] != 'stock_id' else col[0] for col in trades_features.columns.values]\n",
    "\n",
    "merged_features = pd.merge(features, trades_features, on=['time_id', 'stock_id'], how='left')\n",
    "\n",
    "train_merged = train.merge(merged_features, on=['stock_id', 'time_id'], how='left')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-based feature engineering. Extract day of week, week of month, and hour of day. Add a binary indicator for Fed meeting dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_id_reference['day_of_week'] = time_id_reference['date'].dt.dayofweek\n",
    "time_id_reference['week_of_month'] = time_id_reference['date'].apply(lambda x: (x.day - 1) // 7 + 1)\n",
    "time_id_reference['hour_of_day'] = time_id_reference['time'].apply(lambda x: datetime.strptime(x, '%H:%M:%S').hour)\n",
    "\n",
    "# Add Fed meeting dates\n",
    "fed_meeting_dates = [\n",
    "    '2021-01-26', '2021-01-27',\n",
    "    '2021-03-16', '2021-03-17',\n",
    "    '2021-04-27', '2021-04-28',\n",
    "    '2021-06-15', '2021-06-16',\n",
    "    '2021-07-27', '2021-07-28',\n",
    "    '2021-09-21', '2021-09-22'\n",
    "]\n",
    "\n",
    "time_id_reference['fed_meeting'] = time_id_reference['date'].isin(fed_meeting_dates).astype(int)\n",
    "\n",
    "train_merged = train_merged.merge(time_id_reference, on=['time_id'], how='left')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding. One-hot encode 'day_of_week' and 'week_of_month' features and merge the one-hot encoded features with the train data.\n",
    "The one-hot encoding of the day of the week and week of the month creates dummy variables, which allows the model to learn patterns related to specific days and weeks without assuming any ordinal relationship between them. The time of the day is already implicitly included in the model through the time_id. By incorporating these time-related features, the model can learn to adjust its predictions based on the time-related patterns present in the data. If the data shows that certain hours, weekdays, or weeks of the month have higher or lower realized volatility on average, the model will learn these relationships and adjust its predictions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yskakshiyap/.pyenv/versions/3.10.9/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/yskakshiyap/.pyenv/versions/3.10.9/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "day_of_week_one_hot = one_hot_encoder.fit_transform(train_merged[['day_of_week']])\n",
    "week_of_month_one_hot = one_hot_encoder.fit_transform(train_merged[['week_of_month']])\n",
    "\n",
    "day_of_week_columns = [f'day_of_week_{i}' for i in range(day_of_week_one_hot.shape[1])]\n",
    "week_of_month_columns = [f'week_of_month_{i}' for i in range(week_of_month_one_hot.shape[1])]\n",
    "\n",
    "day_of_week_df = pd.DataFrame(day_of_week_one_hot, columns=day_of_week_columns)\n",
    "week_of_month_df = pd.DataFrame(week_of_month_one_hot, columns=week_of_month_columns)\n",
    "\n",
    "train_merged = pd.concat([train_merged, day_of_week_df, week_of_month_df], axis=1)\n",
    "\n",
    "train_merged = train_merged.drop(['date', 'time', 'day_of_week', 'week_of_month'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for the model\n",
    "- Drop unnecessary columns from the train data\n",
    "- Assign sample weights based on whether there was a Fed meeting\n",
    "- Standardize the feature data using StandardScaler\n",
    "- Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = train_merged.drop(['target', 'time_id', 'stock_id'], axis=1)\n",
    "y = train_merged['target']\n",
    "\n",
    "sample_weights = np.where(train_merged['fed_meeting'] == 1, 3, 1)  # Assign a weight of 3 to Fed meeting dates, and 1 otherwise\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training and evaluation\n",
    "- Instantiate an XGBoost Regressor model\n",
    "- Define a parameter grid for hyperparameter tuning\n",
    "- Use RandomizedSearchCV for hyperparameter tuning with cross-validation\n",
    "- Train the model using sample weights\n",
    "- Evaluate the model performance using R2 score and RMSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of the XGBoost model with feature engineering combining both fed meeting dates and special time_id references: R2 score: 0.929, RMSPE: 0.078\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using the sample weights\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "    'min_child_weight': [5, 10, 50, 100],\n",
    "    'subsample': [0.5, 0.75, 1],\n",
    "    'colsample_bytree': [0.5, 0.75, 1],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "\n",
    "random_cv = RandomizedSearchCV(model, param_distributions=param_grid, n_jobs=-1, cv=3, scoring=\"r2\", n_iter=50, random_state=42)\n",
    "random_cv.fit(X_train, y_train, sample_weight=sample_weights[:len(X_train)])\n",
    "\n",
    "best_model = random_cv.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "R2 = round(r2_score(y_true=y_test, y_pred=y_pred), 3)\n",
    "RMSPE = round(rmspe(y_true=y_test, y_pred=y_pred), 3)\n",
    "\n",
    "print(f'Performance of the XGBoost model with feature engineering combining both fed meeting dates and special time_id references: R2 score: {R2}, RMSPE: {RMSPE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.0036606616429689623\n",
      "Mean Squared Error (MSE): 2.580324771988886e-05\n",
      "Root Mean Squared Error (RMSE): 0.0050796897267341885\n",
      "Mean Absolute Percentage Error (MAPE): 0.05968371693309086\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
